{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0PqWb9Y6KjnFVs9NF+qYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbiish/learning_langchain/blob/main/Full_Langchain_Handbook_Workthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [LangChain AI Handbook](https://www.pinecone.io/learn/langchain/) - Full Workthrough  \n",
        "----\n",
        "## Find the repo and colabs [here](https://github.com/nbiish/learning_langchain)"
      ],
      "metadata": {
        "id": "nAm6wK3iQJ0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Enter your api keys here when jumping around/continuing.  \n",
        "\n",
        "#@markdown #### Get your OpenAI key [here](https://beta.openai.com/account/api-keys)..  \n",
        "#@markdown #### ..and your HuggingFace key [here](https://huggingface.co/settings/tokens) \n",
        "\n",
        "import os\n",
        "\n",
        "openai_api_key = '' #@param {type:\"string\"}\n",
        "os.environ['OPENAI_API_TOKEN'] = openai_api_key\n",
        "\n",
        "huggingface_api_token = ''#@param {type:\"string\"}\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_api_token"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IKUebhgjmFo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Core dependencies for jumping around/continuing.  \n",
        "\n",
        "#@markdown ----\n",
        "!pip install -q huggingface_hub langchain openai"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q5VRjpnSmvfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 1*** - LangChain: Introduction and Getting Started](https://www.pinecone.io/learn/langchain-intro/)"
      ],
      "metadata": {
        "id": "6XBIR7RwRAYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***CHAPTER 1*** - Our First PromptTemplate"
      ],
      "metadata": {
        "id": "nDnvFk5TdFMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "Xv5sOvoNN4oU"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Creating Prompts in LangChain\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "question_prompt = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=['question']\n",
        ")\n",
        "\n",
        "question = question_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***CHAPTER 1*** - Hugging Face Hub LLM\n",
        "**NOTE** - *flan_t5 may be outdated*"
      ],
      "metadata": {
        "id": "T6lA6T8UdOa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import huggingface_endpoint\n",
        "#@markdown #### Get your Hugging Face API [here](https://huggingface.co/settings/tokens).\n",
        "\n",
        "import os\n",
        "\n",
        "huggingface_api_token = ''#@param {type:\"string\"}\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_api_token"
      ],
      "metadata": {
        "id": "pd4ghOvdb_Oc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3aLOInfc5KD",
        "outputId": "4c4ad153-10a2-4efb-921d-1f1e924ed528"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/224.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m215.0/224.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "hub_llm = HuggingFaceHub(\n",
        "    repo_id='google/flan-t5-xl',\n",
        "    model_kwargs={'temperature':1e-10}\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=hub_llm\n",
        ")\n",
        "\n",
        "question = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "UZqLfJiLdt3E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***CHAPTER 1*** - Asking Multiple Question"
      ],
      "metadata": {
        "id": "jKE3y_pDklmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This section uses the 'generate' methis to answer the question one at a time.\n",
        "\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2xvKyrAkucY",
        "outputId": "c0372e99-1266-4f0b-8fc3-064864404d5d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='184', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This section sends all of the questions to the LLM as a single prompt\n",
        "\n",
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "\n",
        "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "QgrKZ8aPlxTL",
        "outputId": "9f5be8ab-a37e-439d-8dcf-c9b694fd47f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3b6847ef68e8>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m llm_chain = LLMChain(\n\u001b[1;32m     14\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlong_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflan_t5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'flan_t5' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***CHAPTER 1*** - OenAI LLMs"
      ],
      "metadata": {
        "id": "zbnZ-4JTlLkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "openai_api_key = '' #@param {type:\"string\"}\n",
        "os.environ['OPENAI_API_TOKEN'] = openai_api_key"
      ],
      "metadata": {
        "id": "BeZs2aRqldLK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}