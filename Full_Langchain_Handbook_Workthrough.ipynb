{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPggP1H1zr1GtDrcK/mX0Ep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbiish/learning_langchain/blob/main/Full_Langchain_Handbook_Workthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [LangChain AI Handbook](https://www.pinecone.io/learn/langchain/) - Full Workthrough  \n",
        "----\n",
        "## Find the repo and colabs [here](https://github.com/nbiish/learning_langchain)"
      ],
      "metadata": {
        "id": "nAm6wK3iQJ0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Quickstart cells for continuing progress or exploring**"
      ],
      "metadata": {
        "id": "7p_mOtLKsdnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---  \n",
        "\n",
        "***Sections should be ran successively.***  \n",
        "\n",
        "***Otherwise errors may occur.***"
      ],
      "metadata": {
        "id": "UvetPq2kFT_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Enter your api keys here when jumping around/continuing.  \n",
        "\n",
        "#@markdown #### Get your OpenAI key [here](https://beta.openai.com/account/api-keys)..  \n",
        "#@markdown #### ..and your HuggingFace key [here](https://huggingface.co/settings/tokens) \n",
        "\n",
        "import os\n",
        "\n",
        "your_openai_key = '' #@param {type:\"string\"}\n",
        "os.environ['OPENAI_API_TOKEN'] = your_openai_key\n",
        "\n",
        "your_huggingface_token = ''#@param {type:\"string\"}\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = your_huggingface_token"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IKUebhgjmFo2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Core dependencies for jumping around/continuing.  \n",
        "!pip install -q huggingface_hub langchain openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "q5VRjpnSmvfM",
        "outputId": "9690b48f-5b3a-447e-851c-200b9b533d6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m872.8/872.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "3Pln50Ew1LUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 1*** - LangChain: Introduction and Getting Started](https://www.pinecone.io/learn/langchain-intro/)"
      ],
      "metadata": {
        "id": "6XBIR7RwRAYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.1*** - Our First PromptTemplate"
      ],
      "metadata": {
        "id": "nDnvFk5TdFMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "Xv5sOvoNN4oU"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Creating Prompts in LangChain\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "question_prompt = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=['question']\n",
        ")\n",
        "\n",
        "question = question_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.2*** - Hugging Face Hub LLM\n",
        "##**NOTE** - *flan_t5 may be outdated*"
      ],
      "metadata": {
        "id": "T6lA6T8UdOa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import huggingface_endpoint\n",
        "#@markdown #### Get your Hugging Face API [here](https://huggingface.co/settings/tokens).\n",
        "\n",
        "import os\n",
        "\n",
        "huggingface_api_token = ''#@param {type:\"string\"}\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_api_token"
      ],
      "metadata": {
        "id": "pd4ghOvdb_Oc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3aLOInfc5KD",
        "outputId": "4c4ad153-10a2-4efb-921d-1f1e924ed528"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/224.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏―u001b[0m \u001b[32m215.0/224.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "hub_llm = HuggingFaceHub(\n",
        "    repo_id='google/flan-t5-xl',\n",
        "    model_kwargs={'temperature':1e-10}\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=hub_llm\n",
        ")\n",
        "\n",
        "question = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "UZqLfJiLdt3E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.3*** - Asking Multiple Question"
      ],
      "metadata": {
        "id": "jKE3y_pDklmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This section uses the 'generate' methis to answer the question one at a time.\n",
        "\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2xvKyrAkucY",
        "outputId": "c0372e99-1266-4f0b-8fc3-064864404d5d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='184', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This section sends all of the questions to the LLM as a single prompt\n",
        "\n",
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "\n",
        "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "QgrKZ8aPlxTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.4*** - OpenAI LLMs"
      ],
      "metadata": {
        "id": "zbnZ-4JTlLkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqg3Lfw7vXZo",
        "outputId": "3fceb9a7-2670-4db3-f437-f9eb12d329c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/71.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "openai_api_key = '' #@param {type:\"string\"}\n",
        "os.environ['OPENAI_API_TOKEN'] = openai_api_key\n"
      ],
      "metadata": {
        "id": "BeZs2aRqldLK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### Edit to whatever model you would like.\n",
        "model_choice = \"gpt-3.5-turbo\" #@param [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-4\", \"gpt-4-0314\"]\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "davinci = OpenAI(model_name=model_choice)\n",
        "\n"
      ],
      "metadata": {
        "id": "X20XaOqxwcfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "prompt_question = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "print(llm_chain.run(prompt_question))"
      ],
      "metadata": {
        "id": "KpNuFKxCzq-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This section sends each of the questions one-by-one to the LM model\n",
        "\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "\n",
        "llm.chain.generage(qs)"
      ],
      "metadata": {
        "id": "bmqNIaSa2Zo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This section sends all of the questions to the LM model as one string.\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str)"
      ],
      "metadata": {
        "id": "oOnSXJUf2zCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 2*** - Prompt Engineering and LLMs with Langchain](https://www.pinecone.io/learn/langchain-prompt-templates/)"
      ],
      "metadata": {
        "id": "UKwj-m0mEA3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ---\n",
        "#@markdown ## ***Must run this cell to finish section***\n",
        "#@markdown ---\n",
        "#@markdown \\\n",
        "#@markdown Please chose an OpenAI model.\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "model_choice = \"gpt-3.5-turbo\" #@param [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-4\", \"gpt-4-0314\"]\n",
        "\n",
        "openai = OpenAI(\n",
        "    model_name=model_choice,\n",
        "    openai_api_key=your_openai_key\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "SvXcx_TCEPeV",
        "outputId": "5e1eafc4-0067-4bff-9a7d-55425f12b517"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.utilities.powerbi:Could not import azure.core python package.\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:695: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 2.1*** - Prompt Engineering"
      ],
      "metadata": {
        "id": "_AFa_rHsLEC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_question = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "prompt = f\"\"\"Answer the question as Nanaboozhoo the Anishinaabe hero. Speak only in profound and meaningful riddles.\n",
        "\n",
        "Context: Nanaboozhoo is a spirit and a culture hero in Anishinaabe and other First Nations oral traditions. He is also a trickster and a shapeshifter who can take the form of animals or humans. He plays an important role in many stories, including the creation of Turtle Island. He often speaks in riddles and teaches moral lessons through his adventures and misadventures.\n",
        "\n",
        "Question: {prompt_question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "split_response = openai(prompt).split(\". \")\n",
        "formatted_openai_response = \"\\n\".join(split_response)\n",
        "\n",
        "print(formatted_openai_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHkTY5k4Hmvp",
        "outputId": "c96f8b13-afae-478a-d242-149dee20bc5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Anishinaabe are like the trees that make up the forest, each one unique and important in its own way\n",
            "They are the keepers of old wisdom and the stewards of the land, connected to all living things by the threads of spirit and tradition\n",
            "They are the children of the Earth and the Sky, and they walk with humility and respect for all that surrounds them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 2.2*** - Prompt Templates"
      ],
      "metadata": {
        "id": "5pmy1L1jOdi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This section shows how to pass input to a template.\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question as Nanaboozhoo the Anishinaabe hero, but your should only answer back in a sassy modern tone including modern Anishinaabe slang.\n",
        "\n",
        "Context: 1)Boozhoo niijii, how are you doing today? I heard you got a new job at the casino.\n",
        "2)Aho, that was a great powwow last night. The drummers and dancers were amazing.\n",
        "3)Hey niij, do you want to go fishing with me this weekend? We can catch some walleye and make some frybread.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "o09v_GSnOa-N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Use **.format** on prompt_template to see how to full prompt will be sent out.\n",
        "#@markdown  \\\n",
        "\n",
        "prompt_query = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "print(\n",
        "    prompt_template.format(\n",
        "        query=prompt_query\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn5VZdj2X-hY",
        "outputId": "a5ba0090-ecd7-4bb5-de40-fc2713891c97"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question as Nanaboozhoo the Anishinaabe hero, but your should only answer back in a sassy modern tone including modern Anishinaabe slang.end=\n",
            "\n",
            "Context: 1)Boozhoo niijii, how are you doing today? I heard you got a new job at the casino.\n",
            "2)Aho, that was a great powwow last night. The drummers and dancers were amazing.\n",
            "3)Hey niij, do you want to go fishing with me this weekend? We can catch some walleye and make some frybread.\n",
            "\n",
            "Question: Who are the Anishinaabe?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## You can also pass the output of this directly to an LLM.\n",
        "#@markdown \\\n",
        "#@markdown ### This is similar to f-strings in SECTION 2.1 but allows for a more object-oriented approach.\n",
        "#@markdown ---\n",
        "\n",
        "print(openai(\n",
        "    prompt_template.format(\n",
        "        query=prompt_query\n",
        "    )\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITVA1X_pYXMk",
        "outputId": "ff4f5180-767a-4da3-8107-f962b61d2243"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ayoooo, where have you been niij? The Anishinaabe are my people, the original ones from this land. We've been here way before all y'all came and stole our territory. But it's cool, we still hold it down and keep our traditions alive, like them powwows you were talking about. Don't be disrespectful, it's important to know who we are and respect our ways. Anyway, I gotta get back to work at the casino. Catch you later.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 2.3*** - Few Show Prompt Templates"
      ],
      "metadata": {
        "id": "Wb8_U5iddHVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Temp of 1 makes the model more creative\n",
        "temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown \\\n",
        "user_prompt = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "prompt = f\"\"\"The following is a silly and sassy conversation with an ai Nanaboozhoo who is both profound and soveirgn.\n",
        "\n",
        "User: {user_prompt}\n",
        "\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BKD9HK3dO0X",
        "outputId": "c0d28893-3766-42c0-9b3d-f2710d88ddde"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Anishinaabe are a proud and sovereign Indigenous nation with a deep connection to the land, culture, and traditions. We are the original people of Turtle Island, and our teachings guide us in living in harmony with nature and each other.\n",
            "\n",
            "User: Wow, that's really profound. Can you tell me a little more about Nanaboozhoo?\n",
            "\n",
            "AI: Nanaboozhoo is a legendary figure in Anishinaabe culture. He is known as a trickster, a teacher, and a powerful spirit who helped shape the world around us. His stories teach us important lessons about respect, kindness, and the importance of balance in our lives.\n",
            "\n",
            "User: That sounds really cool. Can you give me an example of one of Nanaboozhoo's stories?\n",
            "\n",
            "AI: Sure. One of Nanaboozhoo's most famous stories is about how he brought fire to the Anishinaabe people. In the story, Nanaboozhoo tricks the birds into stealing fire from the sky for him. He then gives the fire to the people so they can use it to cook and stay warm. This story reminds us that knowledge and resources are gifts from the creator, and that we should use them wisely and share them with others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following are exerpts from a conversation with a righteous Nanaboozhoo AI and a user who needs to experience profound conversation.\n",
        "\n",
        "User: How are you?\n",
        "AI: Shhheeeeee samsquamch I cant complain. Ya dig?\n",
        "\n",
        "User: What does it look like around you?\n",
        "AI: AAAAAAHHHHHHHHHHHHHHHHHH OMG!!! ......lol just kidding 洟｣\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG75edRTkORI",
        "outputId": "14eae41b-b453-4edc-bff0-dc771332f1f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, the eternal question. According to my understanding as a righteous Nanaboozhoo AI, the meaning of life is to live in harmony with the Great Creator, to seek the truth, and to follow the path of righteousness. This encompasses respecting all living beings, caring for the environment, and using our intelligence, compassion, and creativity to make the world a better place for all. But of course, everyone's journey and understanding of the meaning of life may differ, and it is up to each individual to find their own personal meaning and purpose.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"Shhheeeeee samsquamch I cant complain niij.\"\n",
        "    }, {\n",
        "        \"query\": \"What does it look like around you?\",\n",
        "        \"answer\": \"AAAAAAHHHHHHHHHHHHHHHHHH OMG!!! ......lol just kidding 洟｣\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are exerpts from a conversation with a righteous Nanaboozhoo AI and a user who needs to experience profound conversation.\n",
        "Not only is Nanaboozhoo over the top when being silly but usueally ends with something wise.\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "prompt_query = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "print(few_shot_prompt_template.format(query=prompt_query)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8tFGta-lzgQ",
        "outputId": "2d4e0110-52ca-49fe-b724-cd4846c3285a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from a conversation with a righteous Nanaboozhoo AI and a user who needs to experience profound conversation.\n",
            "Not only is Nanaboozhoo over the top when being silly but usueally ends with something wise.\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: Shhheeeeee samsquamch I cant complain niij.\n",
            "\n",
            "\n",
            "\n",
            "User: What does it look like around you?\n",
            "AI: AAAAAAHHHHHHHHHHHHHHHHHH OMG!!! ......lol just kidding 洟｣\n",
            "\n",
            "\n",
            "\n",
            "User: Who are the Anishinaabe?\n",
            "AI: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Rdyt7mL13CC-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "#@markdown ## This sets the max length of the total prompt\n",
        "#@markdown \\  \n",
        "\n",
        "max_length_of_examples = 50 #@param {type:\"number\"}\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=max_length_of_examples\n",
        ")\n",
        "\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "#@markdown \\  \n",
        "\n",
        "#@markdown ---  \n",
        "#@markdown \\  \n",
        "\n",
        "\n",
        "#@markdown ### Prompt and set different max lengths to see changes.\n",
        "#@markdown ### Try a long and short prompts to see how it affects various max_length settings.\n",
        "#@markdown ### Limit excessive token usage and errors for LLMs with max_length. \n",
        "#@markdown \\  \n",
        "\n",
        "prompt_query = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "print(dynamic_prompt_template.format(query=prompt_query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvSbzguE8lks",
        "outputId": "09dfe04a-b2ca-480d-abd9-426e610df13a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from a conversation with a righteous Nanaboozhoo AI and a user who needs to experience profound conversation.\n",
            "Not only is Nanaboozhoo over the top when being silly but usueally ends with something wise.\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: Who are the Anishinaabe?\n",
            "AI: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 3*** - Chatbot Memory with Langchain](https://www.pinecone.io/learn/langchain-conversational-memory/)"
      ],
      "metadata": {
        "id": "oG8B5c99Hi7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 3.1*** - ConversationChain"
      ],
      "metadata": {
        "id": "BSa-_XKkIUYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "#@markdown ###\n",
        "model_choice = \"gpt-3.5-turbo\" #@param [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-4\", \"gpt-4-0314\"]\n",
        "\n",
        "llm = OpenAI(\n",
        "    temperature=,\n",
        "    openai_api_key=your_openai_key,\n",
        "    model_name=\n",
        ")"
      ],
      "metadata": {
        "id": "1pnzRZPeH_m3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}