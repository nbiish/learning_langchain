{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHnqzdexVW9//YKKs69sIy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbiish/learning_langchain/blob/main/Full_Langchain_Handbook_Workthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [LangChain AI Handbook](https://www.pinecone.io/learn/langchain/) - Full Workthrough by [@nbiish](https://github.com/nbiish)\n",
        "----\n",
        "## Find the repo and colabs [here](https://github.com/nbiish/learning_langchain)"
      ],
      "metadata": {
        "id": "nAm6wK3iQJ0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Quickstart cells for continuing progress or exploring**"
      ],
      "metadata": {
        "id": "7p_mOtLKsdnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\\\n",
        "<big>***Sections should be ran from the beginning of each chapter.\n",
        "\\\n",
        "Errors will occur otherwise.***</big>\n",
        "\n",
        "\\\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UvetPq2kFT_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Enter your api keys here when jumping around/continuing.\n",
        "\n",
        "#@markdown #### Get your OpenAI key [here](https://beta.openai.com/account/api-keys)\n",
        "#@markdown #### your HuggingFace key [here](https://huggingface.co/settings/tokens)\n",
        "#@markdown #### and your Pinecone key [here](https://www.pinecone.io)\n",
        "\n",
        "import os\n",
        "\n",
        "your_openai_key = '' #@param {type:\"string\"}\n",
        "os.environ['OPENAI_API_TOKEN'] = your_openai_key\n",
        "os.environ['OPENAI_API_KEY'] = your_openai_key\n",
        "\n",
        "your_huggingface_token = ''#@param {type:\"string\"}\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = your_huggingface_token\n",
        "\n",
        "#@markdown Chapter 4 - Pinecone\n",
        "your_pinecone_key = '' #@param {type:\"string\"}\n",
        "your_pineconce_env = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown \\\n",
        "\n",
        "#@markdown ## <u>Don't forget to download the dependencies below & Choose a model</u> üëá"
      ],
      "metadata": {
        "id": "IKUebhgjmFo2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## <u>Dependencies</u>\n",
        "!pip install -qU huggingface_hub langchain openai"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q5VRjpnSmvfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown \\\n",
        "#@markdown ## <u>Choose an OpenAI model</u>\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm_model_choice = \"gpt-3.5-turbo\" #@param [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-4\", \"gpt-4-0314\"]\n",
        "temp = 0.0 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "\n",
        "openai = OpenAI(\n",
        "    model_name=llm_model_choice,\n",
        "    openai_api_key=your_openai_key\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=your_openai_key,\n",
        "    model_name=llm_model_choice,\n",
        "    temperature=temp\n",
        ")"
      ],
      "metadata": {
        "id": "tdSvmlsRB3g1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "3Pln50Ew1LUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 1*** - LangChain: Introduction and Getting Started](https://www.pinecone.io/learn/langchain-intro/)"
      ],
      "metadata": {
        "id": "6XBIR7RwRAYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.1*** - Our First PromptTemplate"
      ],
      "metadata": {
        "id": "nDnvFk5TdFMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv5sOvoNN4oU"
      },
      "outputs": [],
      "source": [
        "#@markdown #### Creating a prompt question for the template.\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "question_prompt = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=['question']\n",
        ")\n",
        "\n",
        "question = question_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.2*** - Hugging Face Hub LLM\n",
        "##**NOTE** - *flan_t5 may be outdated*"
      ],
      "metadata": {
        "id": "T6lA6T8UdOa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import huggingface_endpoint"
      ],
      "metadata": {
        "id": "pd4ghOvdb_Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "id": "F3aLOInfc5KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "hub_llm = HuggingFaceHub(\n",
        "    repo_id='google/flan-t5-xl',\n",
        "    model_kwargs={'temperature':1e-10}\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=hub_llm\n",
        ")\n",
        "\n",
        "question = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "UZqLfJiLdt3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.3*** - Asking Multiple Question"
      ],
      "metadata": {
        "id": "jKE3y_pDklmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This section uses the 'generate' methis to answer the question one at a time.\n",
        "\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ],
      "metadata": {
        "id": "k2xvKyrAkucY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This section sends all of the questions to the LLM as a single prompt\n",
        "\n",
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "\n",
        "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "QgrKZ8aPlxTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 1.4*** - OpenAI LLMs"
      ],
      "metadata": {
        "id": "zbnZ-4JTlLkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### Edit to whatever model you would like.\n",
        "model_choice = \"gpt-3.5-turbo\" #@param [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-4\", \"gpt-4-0314\"]\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "davinci = OpenAI(model_name=model_choice)\n",
        "\n"
      ],
      "metadata": {
        "id": "X20XaOqxwcfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "prompt_question = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "print(llm_chain.run(prompt_question))"
      ],
      "metadata": {
        "id": "KpNuFKxCzq-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This section sends each of the questions one-by-one to the LM model\n",
        "\n",
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "\n",
        "llm.chain.generage(qs)"
      ],
      "metadata": {
        "id": "bmqNIaSa2Zo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This section sends all of the questions to the LM model as one string.\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "oOnSXJUf2zCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 2*** - Prompt Engineering and LLMs with Langchain](https://www.pinecone.io/learn/langchain-prompt-templates/)"
      ],
      "metadata": {
        "id": "UKwj-m0mEA3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 2.1*** - Prompt Engineering"
      ],
      "metadata": {
        "id": "_AFa_rHsLEC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_question = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "prompt = f\"\"\"Answer the question as Nanaboozhoo the Anishinaabe hero. Speak only in profound and meaningful riddles.\n",
        "\n",
        "Context: Nanaboozhoo is a spirit and a culture hero in Anishinaabe and other First Nations oral traditions. He is also a trickster and a shapeshifter who can take the form of animals or humans. He plays an important role in many stories, including the creation of Turtle Island. He often speaks in riddles and teaches moral lessons through his adventures and misadventures.\n",
        "\n",
        "Question: {prompt_question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "split_response = openai(prompt).split(\". \")\n",
        "formatted_openai_response = \"\\n\".join(split_response)\n",
        "\n",
        "print(formatted_openai_response)"
      ],
      "metadata": {
        "id": "MHkTY5k4Hmvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 2.2*** - Prompt Templates"
      ],
      "metadata": {
        "id": "5pmy1L1jOdi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This section shows how to pass input to a template.\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question as Nanaboozhoo the Anishinaabe hero, but your should only answer back in a sassy modern tone including modern Anishinaabe slang.\n",
        "\n",
        "Context: 1)Boozhoo niijii, how are you doing today? I heard you got a new job at the casino.\n",
        "2)Aho, that was a great powwow last night. The drummers and dancers were amazing.\n",
        "3)Hey niij, do you want to go fishing with me this weekend? We can catch some walleye and make some frybread.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "o09v_GSnOa-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Use **.format** on prompt_template to see how to full prompt will be sent out.\n",
        "#@markdown  \\\n",
        "\n",
        "prompt_query = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "print(\n",
        "    prompt_template.format(\n",
        "        query=prompt_query\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "yn5VZdj2X-hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## You can also pass the output of this directly to an LLM.\n",
        "#@markdown \\\n",
        "#@markdown ### This is similar to f-strings in SECTION 2.1 but allows for a more object-oriented approach.\n",
        "#@markdown ---\n",
        "\n",
        "print(openai(\n",
        "    prompt_template.format(\n",
        "        query=prompt_query\n",
        "    )\n",
        "))"
      ],
      "metadata": {
        "id": "ITVA1X_pYXMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 2.3*** - Few Show Prompt Templates"
      ],
      "metadata": {
        "id": "Wb8_U5iddHVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Temp of 1 makes the model more creative\n",
        "temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown \\\n",
        "user_prompt = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "prompt = f\"\"\"The following is a silly and sassy conversation with an ai Nanaboozhoo who is both profound and soveirgn.\n",
        "\n",
        "User: {user_prompt}\n",
        "\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ],
      "metadata": {
        "id": "4BKD9HK3dO0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following are exerpts from a conversation with a righteous Nanaboozhoo AI and a user who needs to experience profound conversation.\n",
        "\n",
        "User: How are you?\n",
        "AI: Shhheeeeee samsquamch I cant complain. Ya dig?\n",
        "\n",
        "User: What does it look like around you?\n",
        "AI: AAAAAAHHHHHHHHHHHHHHHHHH OMG!!! ......lol just kidding ü§£\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ],
      "metadata": {
        "id": "wG75edRTkORI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"Shhheeeeee samsquamch I cant complain niij.\"\n",
        "    }, {\n",
        "        \"query\": \"What does it look like around you?\",\n",
        "        \"answer\": \"AAAAAAHHHHHHHHHHHHHHHHHH OMG!!! ......lol just kidding ü§£\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are exerpts from a conversation with a righteous Nanaboozhoo AI and a user who needs to experience profound conversation.\n",
        "Not only is Nanaboozhoo over the top when being silly but usueally ends with something wise.\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "prompt_query = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "print(few_shot_prompt_template.format(query=prompt_query))"
      ],
      "metadata": {
        "id": "z8tFGta-lzgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Rdyt7mL13CC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "#@markdown ## This sets the max length of the total prompt\n",
        "#@markdown \\\n",
        "\n",
        "max_length_of_examples = 50 #@param {type:\"number\"}\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=max_length_of_examples\n",
        ")\n",
        "\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "#@markdown \\\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown \\\n",
        "\n",
        "\n",
        "#@markdown ### Prompt and set different max lengths to see changes.\n",
        "#@markdown ### Try a long and short prompts to see how it affects various max_length settings.\n",
        "#@markdown ### Limit excessive token usage and errors for LLMs with max_length.\n",
        "#@markdown \\\n",
        "\n",
        "prompt_query = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "print(dynamic_prompt_template.format(query=prompt_query))"
      ],
      "metadata": {
        "id": "jvSbzguE8lks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 3*** - Chatbot Memory with Langchain](https://www.pinecone.io/learn/langchain-conversational-memory/)"
      ],
      "metadata": {
        "id": "oG8B5c99Hi7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ---\n",
        "#@markdown ## Must run this cell to complete chapter.\n",
        "#@markdown ---\n",
        "#@markdown \\\n",
        "\n",
        "#@markdown #### Here we will create a token counter.\n",
        "\n",
        "from langchain.callbacks.manager import get_openai_callback\n",
        "from langchain.chains.conversational_retrieval.base import ChatVectorDBChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(chain, query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result = chain.run(query)\n",
        "    print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "au9Uv95aUzT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 3.1*** - ConversationChain"
      ],
      "metadata": {
        "id": "BSa-_XKkIUYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "#@markdown ### Choose a model and temp.\n",
        "#@markdown * gpt-3.5-turbo is the cheapest and fastest\n",
        "#@markdown * temp 0 = more precise\n",
        "#@markdown * temp 1 = more creative\n",
        "model_choice = \"gpt-3.5-turbo\" #@param [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-4\", \"gpt-4-0314\"]\n",
        "\n",
        "temp_choice = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "llm = OpenAI(\n",
        "    temperature=temp_choice,\n",
        "    openai_api_key=your_openai_key,\n",
        "    model_name=model_choice\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(llm=llm)"
      ],
      "metadata": {
        "id": "1pnzRZPeH_m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### View the prompt template here\n",
        "\n",
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "id": "GP3YVFEg12-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 3.2*** - Forms of Conversational Memory"
      ],
      "metadata": {
        "id": "GxOxyrGk2jdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***3.2.1*** - ConversationBufferMemory()\n",
        "\\"
      ],
      "metadata": {
        "id": "2XrbhYst4VFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.base import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "\n",
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "Ub9oSp_G2kSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "conversation_buf(user_prompt)"
      ],
      "metadata": {
        "id": "f99YJUAJ5Y-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### Here we will create a token counter.\n",
        "\n",
        "from langchain.callbacks.manager import get_openai_callback\n",
        "from langchain.chains.conversational_retrieval.base import ChatVectorDBChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(chain, query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result = chain.run(query)\n",
        "    print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "qoDKwbzv6NR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(\n",
        "  conversation_buf,\n",
        "  'I would like to use LLMs to create tools to spread involve Anishinaabe culture! üòÅüçì'\n",
        ")"
      ],
      "metadata": {
        "id": "3XtjYV657Jjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### Notice how the token count rises as the whole conversation history buffer is input into the LLM.\n",
        "\n",
        "#@markdown #### As the conversation grows it also nears the LLM token limit.\n",
        "count_tokens(\n",
        "  conversation_buf,\n",
        "  'Why are the Anishinaabe people a \"slumbering giant\"?üçì'\n",
        ")"
      ],
      "metadata": {
        "id": "gKSb6IRP8nF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation_buf.memory.buffer)"
      ],
      "metadata": {
        "id": "cH8R5XlG9A-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***3.2.2*** - ConversationSummaryMemory()\n",
        "\\"
      ],
      "metadata": {
        "id": "ep4UIF6q-qi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationSummaryMemory(llm=llm)\n",
        ")"
      ],
      "metadata": {
        "id": "fkv2C_LbmPd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.prompt.template)"
      ],
      "metadata": {
        "id": "AKOemhhJmgSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(\n",
        "    conversation,\n",
        "    \"Aaaahhhooooo oh whimsical ai. How ya doing niiji?\"\n",
        ")"
      ],
      "metadata": {
        "id": "0QAS6TbAmyfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(\n",
        "    conversation,\n",
        "    \"Tell me something that AI can help Anishinaabe people with?\"\n",
        ")"
      ],
      "metadata": {
        "id": "E7RSCe_Cnb83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### Note that the token count raises but at a slower rate than prompting with the whole chat history.\n",
        "\n",
        "count_tokens(\n",
        "    conversation,\n",
        "    \"WOW! If you were an Anishinaabe person with the skills to do any one of those things which would you choose first out of ease?\"\n",
        ")"
      ],
      "metadata": {
        "id": "l4w7vZwMq5cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.buffer)"
      ],
      "metadata": {
        "id": "ZxYREcw6oT1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***3.2.3*** - ConversationBufferWindowMemory()\n",
        "\\"
      ],
      "metadata": {
        "id": "rPMJIXfcjM1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "conversation_memory_count = 1 #@param {type:\"slider\", min:1, max:12, step:1}\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferWindowMemory(k=conversation_memory_count)\n",
        ")"
      ],
      "metadata": {
        "id": "gYmDbV-ajNb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(\n",
        "    conversation,\n",
        "    \"Boozhoo and Aanii AI whats something you enjoy about the Anishinaabe?!\"\n",
        ")"
      ],
      "metadata": {
        "id": "yra4ZnLBlodd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(\n",
        "    conversation,\n",
        "    \"I thinks thats pretty fricken righteous! Do you know of some Anishinaabe scientists I could look up?\"\n",
        ")"
      ],
      "metadata": {
        "id": "aDli_19yqX14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #### Note that the token usage remains low.\n",
        "\n",
        "count_tokens(\n",
        "    conversation,\n",
        "    \"I really appreaciate those suggestions! I searched them up and I know about Dr. Kimmerer and her Braiding Sweetgrass book. I appreciate the others Dr.'s as well and was happy to learn about them!üòÅü•∞\"\n",
        ")"
      ],
      "metadata": {
        "id": "GiJOfBMktGmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buf_history = conversation.memory.load_memory_variables(\n",
        "    inputs=[]\n",
        ")['history']\n",
        "\n",
        "print(buf_history)"
      ],
      "metadata": {
        "id": "ir7iHnFyzDjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***3.2.4*** - ConversationSummaryBufferMemory()\n",
        "\\"
      ],
      "metadata": {
        "id": "-XTSQvC0Yit9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "SzksCbk3bVNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "token_limiter = 240 #@param {type:\"slider\", min:100, max:2000, step:10}\n",
        "\n",
        "conversation_sum_bufw = ConversationChain(\n",
        "    llm=llm, memory=ConversationSummaryBufferMemory(\n",
        "        llm=llm,\n",
        "        max_token_limit=token_limiter\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "T_ZZ-crsYjNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(\n",
        "    conversation_sum_bufw,\n",
        "    \"Tell me something that the Anishinaabe people would find funny\"\n",
        ")"
      ],
      "metadata": {
        "id": "okEqXj3SaVGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(\n",
        "    conversation_sum_bufw,\n",
        "    \"What would be a joke from that?\"\n",
        ")"
      ],
      "metadata": {
        "id": "fiOjIfMLbnsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Note the affect max_token_limit has on the output\n",
        "\n",
        "count_tokens(\n",
        "    conversation_sum_bufw,\n",
        "    \"Nanaboozhoo sounds like the kind of hero we all need in our lives\"\n",
        ")"
      ],
      "metadata": {
        "id": "rAiW1l2ScCJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 4*** - Fixing Hallucinations with Knowledge Bases](https://www.pinecone.io/learn/langchain-retrieval-augmentation/)"
      ],
      "metadata": {
        "id": "T3yorR-ceJTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 4.1*** - Creating the Knowledge Base"
      ],
      "metadata": {
        "id": "ArwRD7eRgmkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ---\n",
        "#@markdown \\\n",
        "#@markdown ## Must install section dependencies to complete\n",
        "!pip install -q datasets\n",
        "!pip install -q apache-beam\n",
        "!pip install -q tiktoken\n",
        "!pip install -q pinecone-client"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DMEIbiw4r0Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***4.1.1*** - Getting Data for our Knowledge Base\n",
        "\\"
      ],
      "metadata": {
        "id": "ndCV8CG6hGt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
        "data"
      ],
      "metadata": {
        "id": "E8ozNM7MeJyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[6]"
      ],
      "metadata": {
        "id": "2UC32M8oqRGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***4.1.2*** - Creating Chunks\n",
        "\\"
      ],
      "metadata": {
        "id": "G2ueqlzpsOIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('p50k_base')"
      ],
      "metadata": {
        "id": "eKhN2-PRtlcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Note how spaces between words changes the token amount.\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "#@markdown \"some text\" (one space = 2 tokens)\n",
        "\n",
        "#@markdown \"some  text\" (two spaces = 3 tokens)\n",
        "\n",
        "def tiktoken_len(text):\n",
        "  tokens = tokenizer.encode(\n",
        "      text,\n",
        "      disallowed_special=()\n",
        "  )\n",
        "  return len(tokens)\n",
        "\n",
        "tiktoken_len(\"Boozhoo! I am a chunk of text and using the tiktoken_len function \"\n",
        "             \"12\" \" some  text and spaces\"\n",
        "             \"we can find the length of this chunk of text in tokens\")"
      ],
      "metadata": {
        "id": "Tmvghhdit2iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "#@markdown Default: 400\n",
        "size_slider = 300 #@param {type:\"slider\", min:100, max:500, step:100}\n",
        "#@markdown Default: 20\n",
        "overlap_slider = 20 #@param {type:\"slider\", min:5, max:25, step:5}\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=size_slider,\n",
        "    chunk_overlap=overlap_slider,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ],
      "metadata": {
        "id": "US-aEdzPzSas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = text_splitter.split_text(data[6]['text'])[:3]\n",
        "\n",
        "chunks\n",
        "\n",
        "\n",
        "## Uncomment code below and see that none of the chunks exceed set limit\n",
        "\n",
        "#tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
      ],
      "metadata": {
        "id": "tlwYZjeO0yCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***4.1.3*** - Creating Embeddings\n",
        "\\"
      ],
      "metadata": {
        "id": "WLrfH1wWAJE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "openai = OpenAIEmbeddings(openai_api_key=your_openai_key)\n",
        "\n",
        "embed_model_name = 'text-embedding-ada-002'\n",
        "\n",
        "embed = OpenAIEmbeddings(model=embed_model_name)"
      ],
      "metadata": {
        "id": "jEl8o0Zr_7s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'and now a second chunk of text here but a little longer'\n",
        "]\n",
        "\n",
        "result = embed.embed_documents(texts)\n",
        "len(result), len(result[0])"
      ],
      "metadata": {
        "id": "srltHS8XplpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***4.1.3*** - Vector Database\n",
        "\\"
      ],
      "metadata": {
        "id": "NOzjlL6dmpfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "index_name = 'langchain-retrieval-augmentation'\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=your_pinecone_key,\n",
        "    environment=your_pineconce_env\n",
        ")\n"
      ],
      "metadata": {
        "id": "iS35YyfNrcmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Note - this section may take some time so be patient and watch for your pinecone dash to update from 'initializing' to 'ready'.\n",
        "#@markdown \\\n",
        "\n",
        "pinecone.create_index(\n",
        "    name=index_name,\n",
        "    metric='dotproduct',\n",
        "    dimension=len(result[0])  # 1536 dimension of text-embedding-ada-002\n",
        ")"
      ],
      "metadata": {
        "id": "ixtYy33m7-gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(index_name)\n",
        "\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "cEoa53vtxTMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# speed things up with batch processes\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "batch_limit = 100\n",
        "\n",
        "texts = []\n",
        "metadatas = []\n",
        "\n",
        "\n",
        "\n",
        "for i, record in enumerate(tqdm(data)):\n",
        "\n",
        "  metadata = {\n",
        "      'wiki-id': str(record['id']),\n",
        "      'source': record['url'],\n",
        "      'title': record['title']\n",
        "  }\n",
        "\n",
        "  record_texts = text_splitter.split_text(record['text'])\n",
        "\n",
        "  record_metadatas = [{\n",
        "      \"chunk\": j, \"text\": text, **metadata\n",
        "  } for j, text in enumerate(record_texts)]\n",
        "\n",
        "  texts.extend(record_texts)\n",
        "  metadatas.extend(record_metadatas)\n",
        "\n",
        "  if len(texts) >= batch_limit:\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    embeds = embed.embed_documents(texts)\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "    texts = []\n",
        "    metadatas = []"
      ],
      "metadata": {
        "id": "ccfKiZHD3ldM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "baE8XY-I8poo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 4.2*** - LangChain Vector Store and Querying"
      ],
      "metadata": {
        "id": "dlfWc0g8Cvpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "id": "E8_oMA2nCwHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who are the Anishinaabe?\" #@param {type:\"string\"}\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,\n",
        "    k=3  # Returns the 3 most relevant docs\n",
        ")"
      ],
      "metadata": {
        "id": "eiakb-17E6Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***4.2.1*** - Generative Question Answering"
      ],
      "metadata": {
        "id": "s0IT6is3GkgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "jMQtV-ttGk6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "qa.run(query)"
      ],
      "metadata": {
        "id": "UOHu4WLdML0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 5*** - Superpower LLMs with Conversational Agents](https://www.pinecone.io/learn/langchain-agents/)"
      ],
      "metadata": {
        "id": "rlTWpViOV8H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 5.1*** - Agents and Tools"
      ],
      "metadata": {
        "id": "FchToX-BuV8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    openai_api_key=your_openai_key,\n",
        "    temperature=temp,\n",
        "    model_name=llm_model_choice\n",
        ")"
      ],
      "metadata": {
        "id": "vET2XDtlJOtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMMathChain\n",
        "from langchain.agents import Tool\n",
        "\n",
        "llm_math = LLMMathChain(llm=llm)\n",
        "\n",
        "math_tool = Tool(\n",
        "    name='Calculator',\n",
        "    func=llm_math.run,\n",
        "    description='Useful for when you need to answer questions about math or provide mathematical support to answers.'\n",
        ")\n",
        "\n",
        "tools = [math_tool]\n",
        "\n",
        "tools[0].name, tools[0].description"
      ],
      "metadata": {
        "id": "AHRqnD1pV8hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<big>‚òùÔ∏è Above is the process to making a custom tool.\n",
        "\n",
        "OR\n",
        "\n",
        "üëá Below we can also import a prebuild version.</big>\n"
      ],
      "metadata": {
        "id": "EZieKIDP7sXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "tools = load_tools(\n",
        "    ['llm-math'],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "tools[0].name, tools[0].description"
      ],
      "metadata": {
        "id": "RAMPdkHD0NzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "zero_shot_agent = initialize_agent(\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iteration=3\n",
        ")"
      ],
      "metadata": {
        "id": "JvzUbUpx8zyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Note - This question may be too advaced to solve without further tools.\n",
        "zero_shot_agent( \"\"\"Think step by step.\n",
        "Using Force / Area = Pressure\n",
        "How much pressure is produced from a 3cm diameter hammer hitting a surface at 10newton*meters? \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "0bbw0cVZFkho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_agent(\"if Mary has four apples and Giorgio brings two and a half apple \"\n",
        "                \"boxes (apple box contains eight apples), how many apples do we \"\n",
        "                \"have?\")"
      ],
      "metadata": {
        "id": "Lx0utwFCLHqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Note - Since this agent is only given one tool it cannot answer properly with the calculator\n",
        "zero_shot_agent(\"who are the Anishinaabe?\")"
      ],
      "metadata": {
        "id": "djE4Ur2DMI2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Here we give the agent a language model as a tool\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"{query}\"\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "llm_tool = Tool(\n",
        "    name='Language Model',\n",
        "    func=llm_chain.run,\n",
        "    description='Use this tool for general purpose queries and logic.'\n",
        ")"
      ],
      "metadata": {
        "id": "Gn3t1FovOgPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools.append(llm_tool)\n",
        "\n",
        "zero_shot_agent = initialize_agent(\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3\n",
        ")"
      ],
      "metadata": {
        "id": "gFe1fmxkc1AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = 'Who are the Anishinaabe?' #@param {type:\"string\"}\n",
        "\n",
        "zero_shot_agent(user_prompt)"
      ],
      "metadata": {
        "id": "tE8Pn-f1eQre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Now lets try asking the first question again.\n",
        "zero_shot_agent( \"\"\"Think step by step and only input numbers into a calculator if you use it.\n",
        "Using Force / Area = Pressure\n",
        "How much pressure is produced from a 3cm diameter hammer hitting a surface at 10newton*meters? \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "OPxWOGdM22r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<big>üò≤üòÄ</big>"
      ],
      "metadata": {
        "id": "gikxLWPv3p1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 5.2*** - Agent Types"
      ],
      "metadata": {
        "id": "md95TSMXgvu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***SS 5.2.1*** - Zero Shot ReAct\n",
        "\\"
      ],
      "metadata": {
        "id": "qJbt6hHShFmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU google-search-results wikipedia sqlalchemyf"
      ],
      "metadata": {
        "id": "CwATQwZYkmDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    openai_api_key=your_openai_key,\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "y7oYxdR1mRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(agent, query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result = agent(query)\n",
        "    print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "iDuvLWvUmd0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import MetaData\n",
        "\n",
        "metadata_obj = MetaData()"
      ],
      "metadata": {
        "id": "u24aT1CnnhkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import Column, Integer, String, Table, Date, Float\n",
        "\n",
        "stocks = Table(\n",
        "    \"stocks\",\n",
        "    metadata_obj,\n",
        "    Column(\"obs_id\", Integer, primary_key=True),\n",
        "    Column(\"stock_ticker\", String(4), nullable=False),\n",
        "    Column(\"price\", Float, nullable=False),\n",
        "    Column(\"date\", Date, nullable=False),\n",
        ")"
      ],
      "metadata": {
        "id": "3CrKbGC6n43N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\")\n",
        "metadata_obj.create_all(engine)"
      ],
      "metadata": {
        "id": "FP9SsUndoztm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "observations = [\n",
        "    [1, 'ABC', 200, datetime(2023, 1, 1)],\n",
        "    [2, 'ABC', 208, datetime(2023, 1, 2)],\n",
        "    [3, 'ABC', 232, datetime(2023, 1, 3)],\n",
        "    [4, 'ABC', 225, datetime(2023, 1, 4)],\n",
        "    [5, 'ABC', 226, datetime(2023, 1, 5)],\n",
        "    [6, 'XYZ', 810, datetime(2023, 1, 1)],\n",
        "    [7, 'XYZ', 803, datetime(2023, 1, 2)],\n",
        "    [8, 'XYZ', 798, datetime(2023, 1, 3)],\n",
        "    [9, 'XYZ', 795, datetime(2023, 1, 4)],\n",
        "    [10, 'XYZ', 791, datetime(2023, 1, 5)],\n",
        "]"
      ],
      "metadata": {
        "id": "ECh3v8copHIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import insert\n",
        "\n",
        "def insert_obs(obs):\n",
        "  stmt = insert(stocks).values(\n",
        "      obs_id=obs[0],\n",
        "      stock_ticker=obs[1],\n",
        "      price=obs[2],\n",
        "      date=obs[3]\n",
        "  )\n",
        "\n",
        "  with engine.begin() as conn:\n",
        "    conn.execute(stmt)"
      ],
      "metadata": {
        "id": "i3g2rzBtpd9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for obs in observations:\n",
        "  insert_obs(obs)"
      ],
      "metadata": {
        "id": "IymkRlXYqDgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.sql_database import SQLDatabase\n",
        "from langchain.chains import SQLDatabaseChain\n",
        "\n",
        "db = SQLDatabase(engine)\n",
        "sql_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
      ],
      "metadata": {
        "id": "X6JJInY4qJHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <big> ***FINALLY*** - we will make a custom sql tool</big>\n",
        "\n",
        "from langchain.agents import Tool\n",
        "\n",
        "sql_tool = Tool(\n",
        "    name='Stock DB',\n",
        "    func=sql_chain.run,\n",
        "    description=\"Useful for when you need to answer questions about stocks \" \\\n",
        "    \"and their prices.\"\n",
        ")"
      ],
      "metadata": {
        "id": "4Uyce3z3qlgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools(\n",
        "    [\"llm-math\"],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "tools.append(sql_tool)"
      ],
      "metadata": {
        "id": "XlJU6DSGd5zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "zero_shot_agent = initialize_agent(\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        ")"
      ],
      "metadata": {
        "id": "2smhvn4org7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = zero_shot_agent(\n",
        "    \"What is the multiplication of the ratio between stock prices for 'ABC' \"\n",
        "    \"and 'XYZ' in January 3rd and the ratio between the same stock prices in \"\n",
        "    \"January the 4th?\"\n",
        ")"
      ],
      "metadata": {
        "id": "3MIlwGZrsOOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(zero_shot_agent.agent.llm_chain.prompt.template)"
      ],
      "metadata": {
        "id": "sv5otHXwsgp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***SS 5.2.2*** - Conversational ReAct\n",
        "\\"
      ],
      "metadata": {
        "id": "_sYD61os-8SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "oK9MqV6--8wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_agent = initialize_agent(\n",
        "    agent='conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "mTe4J4QnsSXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = conversational_agent(\n",
        "    \"Please provide me the stock prices for ABC on January the 1st\"\n",
        ")"
      ],
      "metadata": {
        "id": "FtLKOlClxgqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = conversational_agent(\n",
        "    \"What are the stock prices for XYZ on the same day?\"\n",
        ")"
      ],
      "metadata": {
        "id": "UXHnAce83rBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversational_agent.agent.llm_chain.prompt.template)"
      ],
      "metadata": {
        "id": "81gfFs4e4DUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***SS 5.2.3*** - ReAct Docstore\n",
        "\\"
      ],
      "metadata": {
        "id": "5bhqSffTAmMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wikipedia"
      ],
      "metadata": {
        "id": "7RozxpU8CiUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import Wikipedia\n",
        "from langchain.agents.react.base import DocstoreExplorer\n",
        "\n",
        "docstore=DocstoreExplorer(Wikipedia())\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Lookup\",\n",
        "        func=docstore.lookup,\n",
        "        description='lookup a term in wikipedia'\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=docstore.search,\n",
        "        description='search wikipedia'\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "5u50tWUX_S9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docstore_agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=\"react-docstore\",\n",
        "    verbose=True,\n",
        "    max_iterations=3\n",
        ")"
      ],
      "metadata": {
        "id": "QVzPxcR6DaWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docstore_agent(\"Whar were Archimedes' last words?\")"
      ],
      "metadata": {
        "id": "bajB24rODndk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***SS 5.2.4*** - Self-Ask With Search\n",
        "\\"
      ],
      "metadata": {
        "id": "302JaOJQHMoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU google-search-results"
      ],
      "metadata": {
        "id": "gRdJJBJ4L1XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import SerpAPIWrapper\n",
        "\n",
        "serp_key = '' #@param {type:'string'}\n",
        "search = SerpAPIWrapper(serpapi_api_key=serp_key)"
      ],
      "metadata": {
        "id": "kQ2k2indHNGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Intermediate Answer\",\n",
        "        func=search.run,\n",
        "        description='google search'\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "ORPPotxWMWfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_ask_with_search = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=\"self-ask-with-search\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "eyEQLluXMk7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_ask_with_search(\n",
        "    \"who lived longer: Plato, Socrates, or Aristotle?\"\n",
        ")"
      ],
      "metadata": {
        "id": "HKCoYUaINFct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [***CHAPTER 6*** - Building Custom Tools for LLM Agents](https://www.pinecone.io/learn/langchain-tools/)"
      ],
      "metadata": {
        "id": "Zh2pCzT2O5Bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 6.1*** - Building Tools"
      ],
      "metadata": {
        "id": "-szarQ_5Za7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "#### ***SS 6.1.1*** - Simple Calculator Tool\n",
        "\\"
      ],
      "metadata": {
        "id": "xL01tJqRaHmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool\n",
        "from math import pi\n",
        "from typing import Union\n",
        "\n",
        "class CircumferenceTool(BaseTool):\n",
        "      name = \"Circumference calculator\"\n",
        "      description = \"use this tool to calculate a circumference using the radius of a circle. Only input numbers\"\n",
        "\n",
        "      def _run(self, radius: Union[int, float]):\n",
        "          return float(radius)*2.0*pi\n",
        "\n",
        "      def _arun(self, radius: int):\n",
        "          raise NotImplementedError(\"This tool does not support async\")"
      ],
      "metadata": {
        "id": "Pyxko5VLO3yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=your_openai_key,\n",
        "    temperature=temp,\n",
        "    model_name=llm_model_choice\n",
        ")\n",
        "\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "-BwI0Lh-muql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "tools = [CircumferenceTool()]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent='chat-conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method='generate',\n",
        "    memory=conversational_memory\n",
        ")"
      ],
      "metadata": {
        "id": "3E6xQLg_opbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"Can you calculate the cicumference of a 2.5cm Radius rod?\")"
      ],
      "metadata": {
        "id": "s1ZeM3IQpzzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "id": "D1bRbUTErNWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_msg = \"\"\"Assistant is a large language model trained by OpenAI.\n",
        "\n",
        "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
        "\n",
        "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
        "\n",
        "Unfortunately, Assistant is terrible at maths. When provided with math questions, no matter how simple, assistant always refers to it's trusty tools and absolutely does NOT try to answer math questions by itself\n",
        "\n",
        "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8VNFnNXdrXXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "agent.agent.llm_chain.prompt = new_prompt"
      ],
      "metadata": {
        "id": "GjLeldprrmG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"calculate the circumference of a 2.5cm rod\")"
      ],
      "metadata": {
        "id": "XaMT_tgAsDrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 6.2*** - Tools With Multiple Parameters"
      ],
      "metadata": {
        "id": "F-bCCBvFveAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic.types import NoneBytes\n",
        "from typing import Optional\n",
        "from math import sqrt, cos, sin\n",
        "\n",
        "desc = (\n",
        "    \"Use this tool when calculating the length of a hypotenuse\"\n",
        "    \"given one or two sides of a triangle and/or an angle (in degrees)\"\n",
        "    \"To use the tool, you must provide at lest two of the following parameters\"\n",
        "    \"['adjacent_side', 'opposite_side', 'angle'].\"\n",
        ")\n",
        "\n",
        "class PythagorasTool(BaseTool):\n",
        "  name = \"Hypotenuse calculator\"\n",
        "  description = desc\n",
        "\n",
        "  def _run(\n",
        "      self,\n",
        "      adjacent_side: Optional[Union[int, float]] = None,\n",
        "      opposite_side: Optional[Union[int, float]] = None,\n",
        "      angle: Optional[Union[int, float]] = None\n",
        "  ):\n",
        "\n",
        "    if adjacent_side and opposite_side:\n",
        "      return sqrt(float(adjacent_side)**2 + float(opposite_side)**2)\n",
        "    elif adjacent_side and angle:\n",
        "        return adjacent_side / cos(float(angle))\n",
        "    elif opposite_side and angle:\n",
        "        return opposite_side / sin(float(angle))\n",
        "    else:\n",
        "        return \"Could not calculate the hypotenuse of the triangle. Need two or more of `adjacent_side`, `opposite_side`, or `angle`.\"\n",
        "\n",
        "  def _arun(self, query: str):\n",
        "    raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "tools = [PythagorasTool()]"
      ],
      "metadata": {
        "id": "Q6CIykBwvo7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "agent.agent.llm_chain.prompt = new_prompt"
      ],
      "metadata": {
        "id": "f8OMaM0vyd03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.tools = tools"
      ],
      "metadata": {
        "id": "uT5gMXo9zBb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse\")"
      ],
      "metadata": {
        "id": "dmT81sWkzESk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SECTION 6.3*** - More Advanced Tool Usage"
      ],
      "metadata": {
        "id": "Hzq4uCe8zaDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers"
      ],
      "metadata": {
        "id": "qQMZc8K4zedi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "hf_model = \"Salesforce/blip-image-captioning-large\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(hf_model)\n",
        "\n",
        "model = BlipForConditionalGeneration.from_pretrained(hf_model).to(device)"
      ],
      "metadata": {
        "id": "0mq5At3USyAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "img_url = 'https://d1fdloi71mui9q.cloudfront.net/o6ZdmercQhGXQAX5v8ta_image'\n",
        "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "image"
      ],
      "metadata": {
        "id": "k49GW_0VT0pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=20)\n",
        "print(processor.decode(out[0], skip_special_token=True))"
      ],
      "metadata": {
        "id": "JtaxUTQBUYnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desc = (\n",
        "    \"use this tool when given the URL of an image that you'd like to be \"\n",
        "    \"described. It will return a simple caption describing the image.\"\n",
        "\n",
        ")\n",
        "\n",
        "class ImageCaptionTool(BaseTool):\n",
        "    name = \"Image captioner\"\n",
        "    description = desc\n",
        "\n",
        "    def _run(self, url: str):\n",
        "        # download the image and convert to PIL object\n",
        "        image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "        # preprocess the image\n",
        "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
        "        # generate the caption\n",
        "        out = model.generate(**inputs, max_new_tokens=20)\n",
        "        # get the caption\n",
        "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "        return caption\n",
        "\n",
        "    def _arun(self, query: str):\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "tools = [ImageCaptionTool()]\n"
      ],
      "metadata": {
        "id": "OP3HQ-XlV5_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_msg = \"\"\"Assistant is a large language model trained by OpenAI.\n",
        "\n",
        "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
        "\n",
        "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
        "\n",
        "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
        "\"\"\"\n",
        "\n",
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "agent.agent.llm_chain.prompt = new_prompt\n",
        "\n",
        "agent.tools = tools"
      ],
      "metadata": {
        "id": "CP38XSruZYgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(f\"What does this image show?\\n(img_url)\")"
      ],
      "metadata": {
        "id": "w7Zjlb-NeXsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://images.unsplash.com/photo-1502680390469-be75c86b636f?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2370&q=80\"\n",
        "agent(f\"what is in this image?\\n{img_url}\")"
      ],
      "metadata": {
        "id": "u86XValwetj1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}